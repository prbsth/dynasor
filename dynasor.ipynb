{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxRe6gi8ix5l",
        "outputId": "4cd80379-4ae4-4b7e-835b-7d8699c7abce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "\u001b[31mERROR: Invalid requirement: 'nvidia-*': Expected end or semicolon (after name and no valid version specifier)\n",
            "    nvidia-*\n",
            "          ^\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m165.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m202.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m181.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m157.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m138.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [torchvision]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m153.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m161.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m140.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m135.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 MB\u001b[0m \u001b[31m143.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m152.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m862.4/862.4 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 MB\u001b[0m \u001b[31m133.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m119.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m913.7/913.7 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67/67\u001b[0m [vllm]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mfatal: destination path 'Dynasor' already exists and is not an empty directory.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building editable for dynasor (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!sudo apt -y install git-lfs             # already present = OK\n",
        "\n",
        "# clean torch that Colab pre‑installed\n",
        "!pip uninstall -y torch torchvision torchaudio nvidia-* || true\n",
        "\n",
        "# ⬇️ 1.  fresh torch+cu121 from the official extra index\n",
        "!pip install --quiet --extra-index-url https://download.pytorch.org/whl/cu121 \\\n",
        "             torch==2.2.1+cu121 torchvision==0.17.1+cu121\n",
        "\n",
        "# ⬇️ 2.  vLLM 0.8.5 (works with 2.2 / cu121–cu122)\n",
        "!pip install --quiet vllm==0.8.5\n",
        "\n",
        "# ⬇️ 3.  Dynasor\n",
        "!git clone https://github.com/hao-ai-lab/Dynasor.git\n",
        "!pip install --quiet -e Dynasor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, vllm, subprocess, platform, os\n",
        "print(torch.__version__, torch.version.cuda)\n",
        "print(\"vllm\", vllm.__version__)\n",
        "!nvcc --version | grep release  # shows CUDA 12.2.x on Colab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usTNxk30jg72",
        "outputId": "aaf838f7-0af5-4cb1-cf97-d3b54015b5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 05-04 04:33:05 [__init__.py:239] Automatically detected platform cuda.\n",
            "2.6.0+cu124 12.4\n",
            "vllm 0.8.5\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export HF_TOKEN=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"
      ],
      "metadata": {
        "id": "I9_wlFc_i451"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login --token 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"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5keLNSVkkq4",
        "outputId": "683cbd29-405a-4872-a676-4ea924a30506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "The token `2241` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `2241`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"openai>=1.52.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtGRwF1MqPcA",
        "outputId": "69b35870-ff8c-47c0-b1e0-43ea5d71680d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai>=1.52.0\n",
            "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>=1.52.0) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.52.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.0) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=1.52.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.52.0) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.52.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.52.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=1.52.0) (0.4.0)\n",
            "Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/662.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.0/662.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.14.3\n",
            "    Uninstalling openai-1.14.3:\n",
            "      Successfully uninstalled openai-1.14.3\n",
            "Successfully installed openai-1.77.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f vllm.entrypoints.openai.api_server || true\n",
        "!pkill -f dynasor-openai || true\n",
        "!pkill -f dynasor-vllm || true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Frwh1O0sZ-m",
        "outputId": "84cfd5f4-d803-4d10-ecc9-1606da6c2855"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "^C\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -s http://127.0.0.1:8000/health && echo \"✅ Dynasor‑vLLM alive\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aydm6ls-vToD",
        "outputId": "7a8dbf00-beec-4b4b-e121-b4c6f686f11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dynasor‑vLLM alive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── launch Dynasor‑vLLM once, then get the prompt back ──\n",
        "import subprocess, shlex, time, atexit, os, sys\n",
        "from contextlib import suppress\n",
        "\n",
        "PORT  = 8000                       # <- one port only\n",
        "MODEL = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
        "ALIAS = \"deepseek-r1\"              # slash‑free alias required for /v1/models\n",
        "\n",
        "cmd = f\"\"\"\n",
        "dynasor-vllm \\\n",
        "  --model {MODEL} \\\n",
        "  --port {PORT} \\\n",
        "  --max-model-len 32768 \\\n",
        "  --enable-prefix-caching \\\n",
        "  --gpu-memory-utilization 0.90 \\\n",
        "  --api-key token-abc123\n",
        "\"\"\"\n",
        "\n",
        "# start the server\n",
        "server_proc = subprocess.Popen(\n",
        "    shlex.split(cmd),\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        ")\n",
        "\n",
        "print(\"⏳ starting dynasor‑vLLM…\")\n",
        "for line in server_proc.stdout:\n",
        "    print(line, end=\"\")            # stream logs for user visibility\n",
        "    if \"Application startup complete.\" in line:\n",
        "        break                      # server is live!\n",
        "\n",
        "server_proc.stdout.close()         # release the pipe so the cell finishes\n",
        "print(f\"✅ server ready on http://127.0.0.1:{PORT}/v1  (model id = {ALIAS})\")\n",
        "\n",
        "# ensure the server dies when the runtime stops / restarts\n",
        "def _cleanup():\n",
        "    with suppress(Exception):\n",
        "        server_proc.terminate()\n",
        "atexit.register(_cleanup)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gHtOP6jewOuF",
        "outputId": "3f8b806f-10db-401d-eb22-140bfd3e9989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ starting dynasor‑vLLM…\n",
            "INFO 05-04 05:25:53 [__init__.py:239] Automatically detected platform cuda.\n",
            "2025-05-04 05:25:53.623656: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1746336353.645101   18404 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1746336353.651689   18404 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "INFO 05-04 05:26:00 [vllm_server.py:1027] vLLM API server version 0.8.5\n",
            "INFO 05-04 05:26:00 [vllm_server.py:1028] args: Namespace(host=None, port=8000, uvicorn_log_level='info', disable_uvicorn_access_log=False, allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='token-abc123', lora_modules=None, prompt_adapters=None, chat_template=None, chat_template_content_format='auto', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, enable_ssl_refresh=False, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_request_id_headers=False, enable_auto_tool_choice=False, tool_call_parser=None, tool_parser_plugin='', model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', task='auto', tokenizer=None, hf_config_path=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path=None, load_format='auto', download_dir=None, model_loader_extra_config={}, use_tqdm_on_load=True, config_format=<ConfigFormat.AUTO: 'auto'>, dtype='auto', max_model_len=32768, guided_decoding_backend='auto', reasoning_parser=None, logits_processor_pattern=None, model_impl='auto', distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, enable_expert_parallel=False, max_parallel_loading_workers=None, ray_workers_use_nsight=False, disable_custom_all_reduce=False, block_size=None, gpu_memory_utilization=0.9, swap_space=4, kv_cache_dtype='auto', num_gpu_blocks_override=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, disable_sliding_window=False, use_v2_block_manager=True, seed=None, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, hf_token=None, hf_overrides=None, enforce_eager=False, max_seq_len_to_capture=8192, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=None, enable_lora_bias=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=None, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', speculative_config=None, ignore_patterns=[], served_model_name=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, max_num_batched_tokens=None, max_num_seqs=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, scheduler_delay_factor=0.0, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, scheduling_policy='fcfs', enable_chunked_prefill=None, disable_chunked_mm_input=False, scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config=None, override_pooler_config=None, compilation_config=None, kv_transfer_config=None, worker_cls='auto', worker_extension_cls='', generation_config='auto', override_generation_config=None, enable_sleep_mode=False, additional_config=None, enable_reasoning=False, disable_cascade_attn=False, disable_log_requests=False, max_log_len=None, disable_fastapi_docs=False, enable_prompt_tokens_details=False, enable_server_load_tracking=False)\n",
            "INFO 05-04 05:26:13 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 05-04 05:26:13 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "WARNING 05-04 05:26:13 [vllm_server.py:171] Could not determine pipeline_parallel_size: 'VllmConfig' object has no attribute 'pipeline_parallel_size'\n",
            "INFO 05-04 05:26:13 [config.py:717] This model supports multiple tasks: {'classify', 'embed', 'score', 'reward', 'generate'}. Defaulting to 'generate'.\n",
            "INFO 05-04 05:26:13 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "INFO 05-04 05:26:15 [core.py:58] Initializing a V1 LLM engine (v0.8.5) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
            "WARNING 05-04 05:26:15 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x784a03433cd0>\n",
            "INFO 05-04 05:26:16 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 05-04 05:26:16 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
            "WARNING 05-04 05:26:16 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
            "INFO 05-04 05:26:16 [gpu_model_runner.py:1329] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B...\n",
            "INFO 05-04 05:26:17 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.20s/it]\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  1.99s/it]\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.02s/it]\n",
            "\n",
            "INFO 05-04 05:26:21 [loader.py:458] Loading weights took 4.16 seconds\n",
            "INFO 05-04 05:26:21 [gpu_model_runner.py:1347] Model loading took 14.2717 GiB and 4.625370 seconds\n",
            "INFO 05-04 05:26:31 [backends.py:420] Using cache directory: /root/.cache/vllm/torch_compile_cache/51a4de2394/rank_0_0 for vLLM's torch.compile\n",
            "INFO 05-04 05:26:31 [backends.py:430] Dynamo bytecode transform time: 9.56 s\n",
            "INFO 05-04 05:26:38 [backends.py:118] Directly load the compiled graph(s) for shape None from the cache, took 6.867 s\n",
            "INFO 05-04 05:26:42 [monitor.py:33] torch.compile takes 9.56 s in total\n",
            "INFO 05-04 05:26:43 [kv_cache_utils.py:634] GPU KV cache size: 362,288 tokens\n",
            "INFO 05-04 05:26:43 [kv_cache_utils.py:637] Maximum concurrency for 32,768 tokens per request: 11.06x\n",
            "INFO 05-04 05:27:15 [gpu_model_runner.py:1686] Graph capturing finished in 32 secs, took 0.48 GiB\n",
            "INFO 05-04 05:27:15 [core.py:159] init engine (profile, create kv cache, warmup model) took 53.70 seconds\n",
            "INFO 05-04 05:27:15 [core_client.py:439] Core engine process 0 ready.\n",
            "INFO 05-04 05:27:15 [vllm_server.py:905] Using supplied chat template:\n",
            "INFO 05-04 05:27:15 [vllm_server.py:905] None\n",
            "WARNING 05-04 05:27:15 [config.py:1239] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "[2025-05-04 05:27:15] INFO serving_chat.py:115: Overwriting default chat sampling param with: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:27:15 [serving_completion.py:61] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.95}\n",
            "[2025-05-04 05:27:15] INFO serving_completion.py:59: Overwriting default completion sampling param with: {'temperature': 0.6, 'top_p': 0.95}\n",
            "INFO 05-04 05:27:15 [launcher.py:28] Available routes are:\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /openapi.json, Methods: GET, HEAD\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /docs, Methods: GET, HEAD\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /redoc, Methods: GET, HEAD\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /health, Methods: GET\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /ping, Methods: GET, POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /tokenize, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /detokenize, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/models, Methods: GET\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /version, Methods: GET\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/completions, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/chat/completions, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/embeddings, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /pooling, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /score, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/score, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /rerank, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v1/rerank, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /v2/rerank, Methods: POST\n",
            "INFO 05-04 05:27:15 [launcher.py:36] Route: /invocations, Methods: POST\n",
            "INFO:     Started server process [18404]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "✅ server ready on http://127.0.0.1:8000/v1  (model id = deepseek-r1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__._cleanup()>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>_cleanup</b><br/>def _cleanup()</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-88-a5e4b96d1da6&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python Dynasor/benchmark/TokenDeprivation/run.py \\\n",
        "  --dataset aime24 \\\n",
        "  --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B \\\n",
        "  --url   http://127.0.0.1:8000/v1 \\\n",
        "  --step 128 \\\n",
        "  --max-tokens 4096 \\\n",
        "  --num-trials 5 \\\n",
        "  --start 0 \\\n",
        "  --end 5 \\\n",
        "  --output /content/aime24_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mX6jGyFewxo9",
        "outputId": "dd69a196-e2fc-49d7-a712-82ed39184f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing question 0 with target [204]\n",
            "Prompt: Every morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Executing question 1 with target [113]\n",
            "Prompt: Let $ABC$ be a triangle inscribed in circle $\\omega$. Let the tangents to $\\omega$ at $B$ and $C$ intersect at point $D$, and let $\\overline{AD}$ intersect $\\omega$ at $P$. If $AB=5$, $BC=9$, and $AC=10$, $AP$ can be written as the form $\\frac{m}{n}$, where $m$ and $n$ are relatively prime integers. Find $m + n$.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Executing question 2 with target [371]\n",
            "Prompt: Each vertex of a regular octagon is independently colored either red or blue with equal probability. The probability that the octagon can then be rotated so that all of the blue vertices end up at positions where there were originally red vertices is $\\tfrac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. What is $m+n$?\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Executing question 3 with target [385]\n",
            "Prompt: Define $f(x)=|| x|-\\tfrac{1}{2}|$ and $g(x)=|| x|-\\tfrac{1}{4}|$. Find the number of intersections of the graphs of \\[y=4 g(f(\\sin (2 \\pi x))) \\quad\\text{ and }\\quad x=4 g(f(\\cos (3 \\pi y))).\\]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Executing question 4 with target [110]\n",
            "Prompt: Let $p$ be the least prime number for which there exists a positive integer $n$ such that $n^{4}+1$ is divisible by $p^{2}$. Find the least positive integer $m$ such that $m^{4}+1$ is divisible by $p^{2}$.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Saved results to /content/aime24_test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rExecuting questions:   0%|          | 0/32 [00:00<?, ?it/s]\rExecuting questions:   3%|▎         | 1/32 [00:02<01:09,  2.24s/it]\rExecuting questions:   6%|▋         | 2/32 [00:04<01:05,  2.17s/it]\rExecuting questions:   9%|▉         | 3/32 [00:06<01:02,  2.15s/it]\rExecuting questions:  12%|█▎        | 4/32 [00:08<00:59,  2.14s/it]\rExecuting questions:  16%|█▌        | 5/32 [00:10<00:57,  2.14s/it]\rExecuting questions:  19%|█▉        | 6/32 [00:12<00:55,  2.15s/it]\rExecuting questions:  22%|██▏       | 7/32 [00:15<00:53,  2.16s/it]\rExecuting questions:  25%|██▌       | 8/32 [00:17<00:51,  2.17s/it]\rExecuting questions:  28%|██▊       | 9/32 [00:19<00:49,  2.17s/it]\rExecuting questions:  31%|███▏      | 10/32 [00:21<00:47,  2.18s/it]\rExecuting questions:  34%|███▍      | 11/32 [00:23<00:45,  2.19s/it]\rExecuting questions:  38%|███▊      | 12/32 [00:26<00:43,  2.20s/it]\rExecuting questions:  41%|████      | 13/32 [00:28<00:41,  2.21s/it]\rExecuting questions:  44%|████▍     | 14/32 [00:30<00:39,  2.21s/it]\rExecuting questions:  47%|████▋     | 15/32 [00:32<00:37,  2.22s/it]\rExecuting questions:  50%|█████     | 16/32 [00:35<00:35,  2.22s/it]\rExecuting questions:  53%|█████▎    | 17/32 [00:38<00:36,  2.46s/it]\rExecuting questions:  56%|█████▋    | 18/32 [00:40<00:34,  2.47s/it]\rExecuting questions:  59%|█████▉    | 19/32 [00:42<00:31,  2.44s/it]\rExecuting questions:  62%|██████▎   | 20/32 [00:45<00:28,  2.38s/it]\rExecuting questions:  66%|██████▌   | 21/32 [00:47<00:25,  2.31s/it]\rExecuting questions:  69%|██████▉   | 22/32 [00:49<00:22,  2.26s/it]\rExecuting questions:  72%|███████▏  | 23/32 [00:51<00:19,  2.21s/it]\rExecuting questions:  75%|███████▌  | 24/32 [00:53<00:17,  2.16s/it]\rExecuting questions:  78%|███████▊  | 25/32 [00:55<00:14,  2.10s/it]\rExecuting questions:  81%|████████▏ | 26/32 [00:56<00:10,  1.73s/it]\rExecuting questions: 100%|██████████| 32/32 [00:56<00:00,  1.76s/it]\n",
            "\rExecuting questions:   0%|          | 0/32 [00:00<?, ?it/s]\rExecuting questions:   3%|▎         | 1/32 [00:02<01:05,  2.11s/it]\rExecuting questions:   6%|▋         | 2/32 [00:04<01:03,  2.13s/it]\rExecuting questions:   9%|▉         | 3/32 [00:06<01:01,  2.13s/it]\rExecuting questions:  12%|█▎        | 4/32 [00:08<00:59,  2.13s/it]\rExecuting questions:  16%|█▌        | 5/32 [00:10<00:57,  2.14s/it]\rExecuting questions:  19%|█▉        | 6/32 [00:12<00:55,  2.15s/it]\rExecuting questions:  22%|██▏       | 7/32 [00:14<00:53,  2.15s/it]\rExecuting questions:  25%|██▌       | 8/32 [00:17<00:52,  2.17s/it]\rExecuting questions:  28%|██▊       | 9/32 [00:19<00:50,  2.17s/it]\rExecuting questions:  31%|███▏      | 10/32 [00:21<00:47,  2.18s/it]\rExecuting questions:  34%|███▍      | 11/32 [00:23<00:45,  2.18s/it]\rExecuting questions:  38%|███▊      | 12/32 [00:25<00:43,  2.19s/it]\rExecuting questions:  41%|████      | 13/32 [00:28<00:41,  2.21s/it]\rExecuting questions:  44%|████▍     | 14/32 [00:30<00:39,  2.22s/it]\rExecuting questions:  47%|████▋     | 15/32 [00:32<00:37,  2.22s/it]\rExecuting questions:  50%|█████     | 16/32 [00:34<00:35,  2.23s/it]\rExecuting questions:  53%|█████▎    | 17/32 [00:37<00:33,  2.24s/it]\rExecuting questions:  56%|█████▋    | 18/32 [00:39<00:31,  2.25s/it]\rExecuting questions:  59%|█████▉    | 19/32 [00:41<00:29,  2.25s/it]\rExecuting questions:  62%|██████▎   | 20/32 [00:44<00:27,  2.26s/it]\rExecuting questions:  66%|██████▌   | 21/32 [00:46<00:24,  2.27s/it]\rExecuting questions:  69%|██████▉   | 22/32 [00:48<00:22,  2.27s/it]\rExecuting questions:  72%|███████▏  | 23/32 [00:50<00:20,  2.28s/it]\rExecuting questions:  75%|███████▌  | 24/32 [00:53<00:18,  2.28s/it]\rExecuting questions:  78%|███████▊  | 25/32 [00:55<00:15,  2.28s/it]\rExecuting questions:  81%|████████▏ | 26/32 [00:57<00:13,  2.29s/it]\rExecuting questions:  84%|████████▍ | 27/32 [01:00<00:11,  2.30s/it]\rExecuting questions:  88%|████████▊ | 28/32 [01:02<00:09,  2.31s/it]\rExecuting questions:  91%|█████████ | 29/32 [01:04<00:06,  2.31s/it]\rExecuting questions:  94%|█████████▍| 30/32 [01:07<00:04,  2.33s/it]\rExecuting questions:  97%|█████████▋| 31/32 [01:09<00:02,  2.35s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.35s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.24s/it]\n",
            "\rExecuting questions:   0%|          | 0/32 [00:00<?, ?it/s]\rExecuting questions:   3%|▎         | 1/32 [00:02<01:05,  2.13s/it]\rExecuting questions:   6%|▋         | 2/32 [00:04<01:03,  2.13s/it]\rExecuting questions:   9%|▉         | 3/32 [00:06<01:01,  2.13s/it]\rExecuting questions:  12%|█▎        | 4/32 [00:08<00:59,  2.14s/it]\rExecuting questions:  16%|█▌        | 5/32 [00:10<00:57,  2.14s/it]\rExecuting questions:  19%|█▉        | 6/32 [00:12<00:55,  2.15s/it]\rExecuting questions:  22%|██▏       | 7/32 [00:15<00:53,  2.16s/it]\rExecuting questions:  25%|██▌       | 8/32 [00:17<00:51,  2.16s/it]\rExecuting questions:  28%|██▊       | 9/32 [00:19<00:49,  2.17s/it]\rExecuting questions:  31%|███▏      | 10/32 [00:21<00:47,  2.18s/it]\rExecuting questions:  34%|███▍      | 11/32 [00:23<00:45,  2.19s/it]\rExecuting questions:  38%|███▊      | 12/32 [00:26<00:44,  2.20s/it]\rExecuting questions:  41%|████      | 13/32 [00:28<00:42,  2.21s/it]\rExecuting questions:  44%|████▍     | 14/32 [00:30<00:39,  2.22s/it]\rExecuting questions:  47%|████▋     | 15/32 [00:32<00:37,  2.23s/it]\rExecuting questions:  50%|█████     | 16/32 [00:34<00:35,  2.23s/it]\rExecuting questions:  53%|█████▎    | 17/32 [00:37<00:33,  2.24s/it]\rExecuting questions:  56%|█████▋    | 18/32 [00:39<00:31,  2.25s/it]\rExecuting questions:  59%|█████▉    | 19/32 [00:41<00:29,  2.25s/it]\rExecuting questions:  62%|██████▎   | 20/32 [00:44<00:27,  2.26s/it]\rExecuting questions:  66%|██████▌   | 21/32 [00:46<00:24,  2.27s/it]\rExecuting questions:  69%|██████▉   | 22/32 [00:48<00:22,  2.27s/it]\rExecuting questions:  72%|███████▏  | 23/32 [00:50<00:20,  2.27s/it]\rExecuting questions:  75%|███████▌  | 24/32 [00:53<00:18,  2.28s/it]\rExecuting questions:  78%|███████▊  | 25/32 [00:55<00:15,  2.28s/it]\rExecuting questions:  81%|████████▏ | 26/32 [00:57<00:13,  2.29s/it]\rExecuting questions:  84%|████████▍ | 27/32 [01:00<00:11,  2.29s/it]\rExecuting questions:  88%|████████▊ | 28/32 [01:02<00:09,  2.30s/it]\rExecuting questions:  91%|█████████ | 29/32 [01:04<00:06,  2.31s/it]\rExecuting questions:  94%|█████████▍| 30/32 [01:07<00:04,  2.33s/it]\rExecuting questions:  97%|█████████▋| 31/32 [01:09<00:02,  2.35s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.37s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.25s/it]\n",
            "\rExecuting questions:   0%|          | 0/32 [00:00<?, ?it/s]\rExecuting questions:   3%|▎         | 1/32 [00:02<01:05,  2.10s/it]\rExecuting questions:   6%|▋         | 2/32 [00:04<01:03,  2.11s/it]\rExecuting questions:   9%|▉         | 3/32 [00:06<01:01,  2.12s/it]\rExecuting questions:  12%|█▎        | 4/32 [00:08<00:59,  2.12s/it]\rExecuting questions:  16%|█▌        | 5/32 [00:10<00:57,  2.13s/it]\rExecuting questions:  19%|█▉        | 6/32 [00:12<00:55,  2.13s/it]\rExecuting questions:  22%|██▏       | 7/32 [00:14<00:53,  2.12s/it]\rExecuting questions:  25%|██▌       | 8/32 [00:17<00:51,  2.13s/it]\rExecuting questions:  28%|██▊       | 9/32 [00:19<00:49,  2.15s/it]\rExecuting questions:  31%|███▏      | 10/32 [00:21<00:47,  2.16s/it]\rExecuting questions:  34%|███▍      | 11/32 [00:23<00:45,  2.16s/it]\rExecuting questions:  38%|███▊      | 12/32 [00:25<00:43,  2.17s/it]\rExecuting questions:  41%|████      | 13/32 [00:27<00:41,  2.18s/it]\rExecuting questions:  44%|████▍     | 14/32 [00:30<00:39,  2.19s/it]\rExecuting questions:  47%|████▋     | 15/32 [00:32<00:37,  2.20s/it]\rExecuting questions:  50%|█████     | 16/32 [00:34<00:35,  2.21s/it]\rExecuting questions:  53%|█████▎    | 17/32 [00:36<00:33,  2.21s/it]\rExecuting questions:  56%|█████▋    | 18/32 [00:39<00:31,  2.22s/it]\rExecuting questions:  59%|█████▉    | 19/32 [00:41<00:28,  2.23s/it]\rExecuting questions:  62%|██████▎   | 20/32 [00:43<00:26,  2.24s/it]\rExecuting questions:  66%|██████▌   | 21/32 [00:45<00:24,  2.26s/it]\rExecuting questions:  69%|██████▉   | 22/32 [00:48<00:22,  2.26s/it]\rExecuting questions:  72%|███████▏  | 23/32 [00:50<00:20,  2.26s/it]\rExecuting questions:  75%|███████▌  | 24/32 [00:52<00:18,  2.27s/it]\rExecuting questions:  78%|███████▊  | 25/32 [00:55<00:15,  2.29s/it]\rExecuting questions:  81%|████████▏ | 26/32 [00:57<00:13,  2.30s/it]\rExecuting questions:  84%|████████▍ | 27/32 [00:59<00:11,  2.30s/it]\rExecuting questions:  88%|████████▊ | 28/32 [01:01<00:09,  2.30s/it]\rExecuting questions:  91%|█████████ | 29/32 [01:04<00:06,  2.30s/it]\rExecuting questions:  94%|█████████▍| 30/32 [01:06<00:04,  2.32s/it]\rExecuting questions:  97%|█████████▋| 31/32 [01:08<00:02,  2.34s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.34s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.23s/it]\n",
            "\rExecuting questions:   0%|          | 0/32 [00:00<?, ?it/s]\rExecuting questions:   3%|▎         | 1/32 [00:02<01:05,  2.11s/it]\rExecuting questions:   6%|▋         | 2/32 [00:04<01:03,  2.11s/it]\rExecuting questions:   9%|▉         | 3/32 [00:06<01:01,  2.12s/it]\rExecuting questions:  12%|█▎        | 4/32 [00:08<00:59,  2.13s/it]\rExecuting questions:  16%|█▌        | 5/32 [00:10<00:57,  2.14s/it]\rExecuting questions:  19%|█▉        | 6/32 [00:12<00:55,  2.14s/it]\rExecuting questions:  22%|██▏       | 7/32 [00:14<00:53,  2.15s/it]\rExecuting questions:  25%|██▌       | 8/32 [00:17<00:51,  2.15s/it]\rExecuting questions:  28%|██▊       | 9/32 [00:19<00:49,  2.16s/it]\rExecuting questions:  31%|███▏      | 10/32 [00:21<00:47,  2.17s/it]\rExecuting questions:  34%|███▍      | 11/32 [00:23<00:45,  2.18s/it]\rExecuting questions:  38%|███▊      | 12/32 [00:25<00:43,  2.19s/it]\rExecuting questions:  41%|████      | 13/32 [00:28<00:41,  2.19s/it]\rExecuting questions:  44%|████▍     | 14/32 [00:30<00:39,  2.21s/it]\rExecuting questions:  47%|████▋     | 15/32 [00:32<00:37,  2.22s/it]\rExecuting questions:  50%|█████     | 16/32 [00:34<00:35,  2.22s/it]\rExecuting questions:  53%|█████▎    | 17/32 [00:37<00:33,  2.22s/it]\rExecuting questions:  56%|█████▋    | 18/32 [00:39<00:31,  2.23s/it]\rExecuting questions:  59%|█████▉    | 19/32 [00:41<00:29,  2.24s/it]\rExecuting questions:  62%|██████▎   | 20/32 [00:43<00:26,  2.25s/it]\rExecuting questions:  66%|██████▌   | 21/32 [00:46<00:24,  2.25s/it]\rExecuting questions:  69%|██████▉   | 22/32 [00:48<00:22,  2.26s/it]\rExecuting questions:  72%|███████▏  | 23/32 [00:50<00:20,  2.26s/it]\rExecuting questions:  75%|███████▌  | 24/32 [00:52<00:18,  2.27s/it]\rExecuting questions:  78%|███████▊  | 25/32 [00:55<00:15,  2.28s/it]\rExecuting questions:  81%|████████▏ | 26/32 [00:57<00:13,  2.29s/it]\rExecuting questions:  84%|████████▍ | 27/32 [00:59<00:11,  2.29s/it]\rExecuting questions:  88%|████████▊ | 28/32 [01:02<00:09,  2.29s/it]\rExecuting questions:  91%|█████████ | 29/32 [01:04<00:06,  2.29s/it]\rExecuting questions:  94%|█████████▍| 30/32 [01:06<00:04,  2.31s/it]\rExecuting questions:  97%|█████████▋| 31/32 [01:09<00:02,  2.33s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.34s/it]\rExecuting questions: 100%|██████████| 32/32 [01:11<00:00,  2.23s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/file.zip /content/aime24_test/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpne_YO_zAr_",
        "outputId": "aa25640d-36b5-4e1c-e66f-37572f2d6210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/aime24_test/ (stored 0%)\n",
            "  adding: content/aime24_test/question_2_tokens_2176.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_4096.json (deflated 73%)\n",
            "  adding: content/aime24_test/question_4_tokens_3072.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_1_tokens_3968.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_1_tokens_128.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_1_tokens_2816.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_512.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_2816.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_2_tokens_384.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_2_tokens_2304.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_1408.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_1_tokens_2176.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_2_tokens_3072.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_768.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_4_tokens_3456.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_4_tokens_1024.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_1_tokens_2560.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_0_tokens_3072.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_4_tokens_3200.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_1_tokens_1920.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_3_tokens_1664.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_2816.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_2_tokens_3712.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_128.json (deflated 90%)\n",
            "  adding: content/aime24_test/question_0_tokens_768.json (deflated 89%)\n",
            "  adding: content/aime24_test/question_1_tokens_2304.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_1920.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_2_tokens_1792.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_2304.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_3_tokens_3328.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_1792.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_0_tokens_2048.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_2_tokens_2048.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_0_tokens_1280.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_3_tokens_3200.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_512.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_1_tokens_384.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_1280.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_2_tokens_3584.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_3_tokens_1024.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_3_tokens_3584.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_1024.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_0_tokens_2688.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_4_tokens_384.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_2432.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_3840.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_1152.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_768.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_1_tokens_1664.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_0_tokens_1152.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_3_tokens_2944.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_2_tokens_3840.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_2688.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_2_tokens_256.json (deflated 89%)\n",
            "  adding: content/aime24_test/question_0_tokens_2560.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_1_tokens_4096.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_0_tokens_1536.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_1_tokens_640.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_384.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_4_tokens_1536.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_2_tokens_3456.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_256.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_4096.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_3_tokens_2688.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_1792.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_1536.json (deflated 81%)\n",
            "  adding: content/aime24_test/question_0_tokens_3840.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_3_tokens_3712.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_1536.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_0_tokens_2432.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_512.json (deflated 89%)\n",
            "  adding: content/aime24_test/question_4_tokens_3328.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_1_tokens_1280.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_2176.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_1280.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_4_tokens_896.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_2_tokens_512.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_2_tokens_768.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_1_tokens_1152.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_2_tokens_1920.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_2048.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_3_tokens_2048.json (deflated 81%)\n",
            "  adding: content/aime24_test/question_0_tokens_1408.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_0_tokens_256.json (deflated 90%)\n",
            "  adding: content/aime24_test/question_2_tokens_1024.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_3_tokens_1152.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_1_tokens_2688.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_2176.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_3_tokens_512.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_2_tokens_2688.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_4096.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_1_tokens_2944.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_1664.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_2_tokens_896.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_0_tokens_896.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_3_tokens_3968.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_3_tokens_3072.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_3200.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_1_tokens_1024.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_1920.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_1152.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_1_tokens_896.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_4_tokens_2560.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_0_tokens_3968.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_640.json (deflated 89%)\n",
            "  adding: content/aime24_test/question_2_tokens_128.json (deflated 90%)\n",
            "  adding: content/aime24_test/question_2_tokens_2432.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_2816.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_2_tokens_2560.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_4_tokens_128.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_0_tokens_2944.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_3_tokens_2560.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_2_tokens_1664.json (deflated 80%)\n",
            "  adding: content/aime24_test/question_4_tokens_2944.json (deflated 74%)\n",
            "  adding: content/aime24_test/question_3_tokens_2304.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_3328.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_2816.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_2176.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_1_tokens_256.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_4_tokens_3584.json (deflated 73%)\n",
            "  adding: content/aime24_test/question_3_tokens_1792.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_3200.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_1920.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_2_tokens_3328.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_3200.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_3584.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_3_tokens_1280.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_3_tokens_128.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_4_tokens_3840.json (deflated 73%)\n",
            "  adding: content/aime24_test/question_3_tokens_1536.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_0_tokens_3456.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_0_tokens_384.json (deflated 89%)\n",
            "  adding: content/aime24_test/question_2_tokens_640.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_2_tokens_2944.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_1_tokens_3456.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_1_tokens_3712.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_3_tokens_3456.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_3_tokens_1408.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_3_tokens_640.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_896.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_1_tokens_3584.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_1_tokens_1408.json (deflated 87%)\n",
            "  adding: content/aime24_test/question_2_tokens_3968.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_1792.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_4_tokens_2432.json (deflated 85%)\n",
            "  adding: content/aime24_test/question_1_tokens_2432.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_1_tokens_2048.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_2_tokens_1408.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_1_tokens_3072.json (deflated 78%)\n",
            "  adding: content/aime24_test/question_4_tokens_640.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_3_tokens_256.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_0_tokens_2304.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_4_tokens_3968.json (deflated 73%)\n",
            "  adding: content/aime24_test/question_2_tokens_4096.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_3_tokens_3840.json (deflated 79%)\n",
            "  adding: content/aime24_test/question_0_tokens_1664.json (deflated 88%)\n",
            "  adding: content/aime24_test/question_1_tokens_768.json (deflated 86%)\n",
            "  adding: content/aime24_test/question_4_tokens_3712.json (deflated 73%)\n",
            "  adding: content/aime24_test/question_0_tokens_3712.json (deflated 82%)\n",
            "  adding: content/aime24_test/question_1_tokens_3328.json (deflated 78%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lElif2uKzGrP",
        "outputId": "ece1c0ae-0818-4769-b93a-fe0a83596636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4a110724-8c8e-426e-b842-86293868b6ee\", \"file.zip\", 2102811)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}